#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GÃœNLÃœK TOPLAM ENERJÄ° TAHMÄ°NÄ° - BÃ–LÃœM 1/3
TÃ¼m sistem iÃ§in gÃ¼nlÃ¼k toplam enerji tÃ¼ketimi tahmini
"""

import sys
import os
import math
import time
import json
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# MLflow imports
import mlflow
import mlflow.spark
from mlflow.models.signature import infer_signature

# Spark imports
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, sum as spark_sum, avg as spark_avg, max as spark_max, min as spark_min,
    date_format, dayofweek, month, quarter, hour, minute, year,
    when, lit, sin, cos, count, lag, rand, stddev, weekofyear,
    datediff, to_date, date_add, expr
)
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, DateType
from pyspark.sql.window import Window
from pyspark.ml.feature import VectorAssembler, StandardScaler, OneHotEncoder, StringIndexer
from pyspark.ml.regression import GBTRegressor, RandomForestRegressor, LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml import Pipeline

def setup_mlflow_daily_total():
    """
    GÃ¼nlÃ¼k toplam enerji tahmini iÃ§in MLflow setup
    """
    print("ğŸ“Š MLflow GÃ¼nlÃ¼k Toplam Enerji Tahmini Kurulumu...")
    
    tracking_uri = "http://localhost:5000"  # MLflow server URI
    mlflow.set_tracking_uri(tracking_uri)
    
    experiment_name = "Daily_Total_Energy_Forecasting"
    try:
        experiment_id = mlflow.create_experiment(experiment_name)
        print(f"âœ… Yeni deney oluÅŸturuldu: {experiment_name}")
    except:
        experiment = mlflow.get_experiment_by_name(experiment_name)
        experiment_id = experiment.experiment_id
        print(f"âœ… Mevcut deney kullanÄ±lÄ±yor: {experiment_name}")
    
    mlflow.set_experiment(experiment_name)
    
    print(f"ğŸ“ MLflow tracking URI: {tracking_uri}")
    print(f"ğŸ§ª Deney: {experiment_name}")
    print(f"ğŸ†” Deney ID: {experiment_id}")
    
    return tracking_uri, experiment_name, experiment_id

def create_spark_session():
    """
    Optimize edilmiÅŸ Spark session oluÅŸtur
    """
    print("\nğŸš€ Spark Session OluÅŸturuluyor...")
    
    spark = SparkSession.builder \
        .appName("Daily_Total_Energy_Prediction") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.sql.adaptive.skewJoin.enabled", "true") \
        .config("spark.sql.adaptive.localShuffleReader.enabled", "true") \
        .config("spark.sql.adaptive.advisoryPartitionSizeInBytes", "128MB") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .config("spark.sql.shuffle.partitions", "200") \
        .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
        .getOrCreate()
    
    print(f"âœ… Spark {spark.version} baÅŸlatÄ±ldÄ±")
    return spark

def load_and_aggregate_data(spark, data_path):
    """
    Veriyi yÃ¼kle ve gÃ¼nlÃ¼k toplam enerji tÃ¼ketimini hesapla
    """
    print("\nğŸ“‚ Veri YÃ¼kleniyor ve GÃ¼nlÃ¼k Toplamlar HesaplanÄ±yor...")
    
    # CSV dosyalarÄ±nÄ± oku
    df = spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .option("compression", "gzip") \
        .csv(data_path)
    
    # Timestamp kolonunu dÃ¼zelt
    df = df.withColumn("full_timestamp", col("full_timestamp").cast(TimestampType()))
    
    # Tarih kolonu ekle
    df = df.withColumn("date", to_date(col("full_timestamp")))
    
    # GÃ¼nlÃ¼k toplam enerji tÃ¼ketimini hesapla
    daily_total = df.groupBy("date").agg(
        spark_sum("load_percentage").alias("total_daily_energy"),
        count("*").alias("reading_count"),
        spark_avg("load_percentage").alias("avg_load"),
        spark_max("load_percentage").alias("max_load"),
        spark_min("load_percentage").alias("min_load"),
        stddev("load_percentage").alias("load_std"),
        count(when(col("load_percentage") > 80, 1)).alias("high_load_count"),
        count(when(col("load_percentage") < 20, 1)).alias("low_load_count")
    ).orderBy("date")
    
    # Veri kalitesi kontrolÃ¼
    total_days = daily_total.count()
    min_date = daily_total.select(spark_min("date")).collect()[0][0]
    max_date = daily_total.select(spark_max("date")).collect()[0][0]
    
    print(f"âœ… Toplam gÃ¼n sayÄ±sÄ±: {total_days}")
    print(f"ğŸ“… Tarih aralÄ±ÄŸÄ±: {min_date} - {max_date}")
    print(f"ğŸ“Š Ortalama gÃ¼nlÃ¼k enerji: {daily_total.select(spark_avg('total_daily_energy')).collect()[0][0]:.2f}")
    
    return daily_total

def create_time_features(df):
    """
    Zaman bazlÄ± Ã¶zellikler oluÅŸtur
    """
    print("\nğŸ”§ Zaman Ã–zellikleri OluÅŸturuluyor...")
    
    # Temel zaman Ã¶zellikleri
    df = df.withColumn("year", year("date")) \
           .withColumn("month", month("date")) \
           .withColumn("day_of_month", dayofweek("date")) \
           .withColumn("day_of_week", dayofweek("date")) \
           .withColumn("week_of_year", weekofyear("date")) \
           .withColumn("quarter", quarter("date")) \
           .withColumn("is_weekend", when(col("day_of_week").isin([1, 7]), 1).otherwise(0)) \
           .withColumn("is_monday", when(col("day_of_week") == 2, 1).otherwise(0)) \
           .withColumn("is_friday", when(col("day_of_week") == 6, 1).otherwise(0))
    
    # Mevsimsel Ã¶zellikler
    df = df.withColumn("season", 
        when(col("month").isin([12, 1, 2]), 0)  # KÄ±ÅŸ
        .when(col("month").isin([3, 4, 5]), 1)  # Ä°lkbahar
        .when(col("month").isin([6, 7, 8]), 2)  # Yaz
        .otherwise(3))  # Sonbahar
    
    # DÃ¶ngÃ¼sel Ã¶zellikler (sin/cos)
    PI = lit(math.pi)
    df = df.withColumn("sin_day_of_week", sin(col("day_of_week") * (2.0 * PI / 7))) \
           .withColumn("cos_day_of_week", cos(col("day_of_week") * (2.0 * PI / 7))) \
           .withColumn("sin_day_of_month", sin(col("day_of_month") * (2.0 * PI / 31))) \
           .withColumn("cos_day_of_month", cos(col("day_of_month") * (2.0 * PI / 31))) \
           .withColumn("sin_month", sin(col("month") * (2.0 * PI / 12))) \
           .withColumn("cos_month", cos(col("month") * (2.0 * PI / 12))) \
           .withColumn("sin_week_of_year", sin(col("week_of_year") * (2.0 * PI / 52))) \
           .withColumn("cos_week_of_year", cos(col("week_of_year") * (2.0 * PI / 52)))
    
    # Tatil gÃ¼nleri (basit yaklaÅŸÄ±m - TÃ¼rkiye iÃ§in Ã¶zelleÅŸtirilebilir)
    df = df.withColumn("is_holiday", 
        when(
            # YÄ±lbaÅŸÄ±
            ((col("month") == 1) & (col("day_of_month") == 1)) |
            # 23 Nisan
            ((col("month") == 4) & (col("day_of_month") == 23)) |
            # 1 MayÄ±s
            ((col("month") == 5) & (col("day_of_month") == 1)) |
            # 19 MayÄ±s
            ((col("month") == 5) & (col("day_of_month") == 19)) |
            # 30 AÄŸustos
            ((col("month") == 8) & (col("day_of_month") == 30)) |
            # 29 Ekim
            ((col("month") == 10) & (col("day_of_month") == 29)),
            1
        ).otherwise(0))
    
    # Ay baÅŸÄ±/sonu Ã¶zellikleri
    df = df.withColumn("is_month_start", when(col("day_of_month") <= 5, 1).otherwise(0)) \
           .withColumn("is_month_end", when(col("day_of_month") >= 26, 1).otherwise(0))
    
    print("âœ… Zaman Ã¶zellikleri oluÅŸturuldu")
    return df

def create_lag_features(df):
    """
    Lag (gecikmeli) Ã¶zellikler oluÅŸtur - Ã‡ok Ã¶nemli!
    """
    print("\nğŸ“ˆ Lag Ã–zellikleri OluÅŸturuluyor...")
    
    # Window specification - tarih sÄ±rasÄ±na gÃ¶re
    window_spec = Window.orderBy("date")
    
    # Ã–nceki gÃ¼nlerin enerji tÃ¼ketimi
    for i in [1, 2, 3, 7, 14, 21, 28]:  # 1, 2, 3, 7, 14, 21, 28 gÃ¼n Ã¶ncesi
        df = df.withColumn(f"energy_lag_{i}d", lag("total_daily_energy", i).over(window_spec))
    
    # Hareketli ortalamalar
    for window_size in [3, 7, 14, 28]:
        df = df.withColumn(
            f"energy_ma_{window_size}d",
            spark_avg("total_daily_energy").over(
                window_spec.rowsBetween(-window_size, -1)
            )
        )
    
    # Hareketli standart sapma
    for window_size in [7, 14]:
        df = df.withColumn(
            f"energy_std_{window_size}d",
            stddev("total_daily_energy").over(
                window_spec.rowsBetween(-window_size, -1)
            )
        )
    
    # Ã–nceki haftanÄ±n aynÄ± gÃ¼nÃ¼
    df = df.withColumn("energy_same_day_last_week", lag("total_daily_energy", 7).over(window_spec))
    
    # Ã–nceki ayÄ±n aynÄ± gÃ¼nÃ¼ (yaklaÅŸÄ±k)
    df = df.withColumn("energy_same_day_last_month", lag("total_daily_energy", 30).over(window_spec))
    
    # Trend Ã¶zellikleri
    df = df.withColumn("energy_trend_7d", 
        (col("total_daily_energy") - lag("total_daily_energy", 7).over(window_spec)) / 
        (lag("total_daily_energy", 7).over(window_spec) + 0.001)
    )
    
    df = df.withColumn("energy_trend_28d",
        (col("total_daily_energy") - lag("total_daily_energy", 28).over(window_spec)) /
        (lag("total_daily_energy", 28).over(window_spec) + 0.001)
    )
    
    # YÃ¼k profili Ã¶zellikleri (Ã¶nceki gÃ¼nlerden)
    for i in [1, 7]:
        df = df.withColumn(f"avg_load_lag_{i}d", lag("avg_load", i).over(window_spec))
        df = df.withColumn(f"max_load_lag_{i}d", lag("max_load", i).over(window_spec))
        df = df.withColumn(f"load_std_lag_{i}d", lag("load_std", i).over(window_spec))
    
    # NULL deÄŸerleri doldur
    lag_columns = [col for col in df.columns if 'lag' in col or 'ma_' in col or 'std_' in col or 'trend' in col]
    for col_name in lag_columns:
        df = df.withColumn(col_name, when(col(col_name).isNull(), 0).otherwise(col(col_name)))
    
    print(f"âœ… {len(lag_columns)} lag Ã¶zelliÄŸi oluÅŸturuldu")
    return df

def create_advanced_features(df):
    """
    Ä°leri dÃ¼zey Ã¶zellikler oluÅŸtur
    """
    print("\nğŸ”¬ Ä°leri DÃ¼zey Ã–zellikler OluÅŸturuluyor...")
    
    # YÄ±lÄ±n gÃ¼nÃ¼
    df = df.withColumn("day_of_year", datediff(col("date"), 
        to_date(lit(f"{df.select(year('date')).first()[0]}-01-01"), "yyyy-MM-dd")) + 1)
    
    # Ã–zel gÃ¼nler arasÄ± mesafe
    df = df.withColumn("days_since_month_start", col("day_of_month") - 1)
    df = df.withColumn("days_until_month_end", 
        datediff(date_add(col("date"), 31 - col("day_of_month")), col("date")))
    
    # Hafta iÃ§i ardÄ±ÅŸÄ±k gÃ¼nler
    df = df.withColumn("consecutive_weekdays",
        when(col("is_weekend") == 0, 
            spark_sum(when(col("is_weekend") == 0, 1).otherwise(0)).over(
                Window.orderBy("date").rowsBetween(Window.unboundedPreceding, 0)
            )
        ).otherwise(0)
    )
    
    # Enerji tÃ¼ketimi kategorileri
    avg_energy = df.select(spark_avg("total_daily_energy")).collect()[0][0]
    std_energy = df.select(stddev("total_daily_energy")).collect()[0][0]
    
    df = df.withColumn("energy_category",
        when(col("total_daily_energy") > avg_energy + 2 * std_energy, "very_high")
        .when(col("total_daily_energy") > avg_energy + std_energy, "high")
        .when(col("total_daily_energy") < avg_energy - std_energy, "low")
        .when(col("total_daily_energy") < avg_energy - 2 * std_energy, "very_low")
        .otherwise("normal")
    )
    
    # Ã–nceki gÃ¼nÃ¼n kategorisi
    window_spec = Window.orderBy("date")
    df = df.withColumn("prev_energy_category", lag("energy_category", 1).over(window_spec))
    
    # Anomali skoru (basit z-score)
    df = df.withColumn("energy_zscore",
        (col("total_daily_energy") - avg_energy) / std_energy
    )
    
    print("âœ… Ä°leri dÃ¼zey Ã¶zellikler oluÅŸturuldu")
    return df

def prepare_train_test_split(df, test_days=30):
    """
    Zaman serisi iÃ§in train/validation/test split
    """
    print(f"\nğŸ”€ Veri BÃ¶lÃ¼nÃ¼yor (Son {test_days} gÃ¼n test iÃ§in)...")
    
    # Son tarih
    max_date = df.select(spark_max("date")).collect()[0][0]
    
    # Test baÅŸlangÄ±Ã§ tarihi
    test_start_date = max_date - timedelta(days=test_days)
    val_start_date = test_start_date - timedelta(days=test_days)
    
    # Veriyi bÃ¶l
    train_df = df.filter(col("date") < val_start_date)
    val_df = df.filter((col("date") >= val_start_date) & (col("date") < test_start_date))
    test_df = df.filter(col("date") >= test_start_date)
    
    # Cache'le
    train_df = train_df.cache()
    val_df = val_df.cache()
    test_df = test_df.cache()
    
    print(f"âœ… Train: {train_df.count()} gÃ¼n")
    print(f"âœ… Validation: {val_df.count()} gÃ¼n")
    print(f"âœ… Test: {test_df.count()} gÃ¼n")
    
    return train_df, val_df, test_df

def build_ml_pipeline(feature_columns, target_column="total_daily_energy"):
    """
    ML pipeline oluÅŸtur
    """
    print("\nğŸ—ï¸ ML Pipeline OluÅŸturuluyor...")
    
    # Kategorik kolonlarÄ± encode et
    categorical_cols = ["energy_category", "prev_energy_category"]
    
    # String Indexer
    indexers = []
    for cat_col in categorical_cols:
        if cat_col in feature_columns:
            indexer = StringIndexer(
                inputCol=cat_col,
                outputCol=f"{cat_col}_indexed",
                handleInvalid="keep"
            )
            indexers.append(indexer)
            # Feature listesini gÃ¼ncelle
            feature_columns = [f"{cat_col}_indexed" if col == cat_col else col for col in feature_columns]
    
    # Numeric features
    numeric_features = [col for col in feature_columns if not col.endswith("_indexed")]
    
    # Vector Assembler
    assembler = VectorAssembler(
        inputCols=feature_columns,
        outputCol="features",
        handleInvalid="skip"
    )
    
    # Scaler
    scaler = StandardScaler(
        inputCol="features",
        outputCol="scaled_features",
        withStd=True,
        withMean=True
    )
    
    # Model - Gradient Boosted Trees
    gbt = GBTRegressor(
        featuresCol="scaled_features",
        labelCol=target_column,
        predictionCol="prediction",
        maxIter=100,
        maxDepth=8,
        stepSize=0.1,
        subsamplingRate=0.8,
        featureSubsetStrategy="sqrt",
        minInstancesPerNode=5,
        maxBins=32
    )
    
    # Pipeline
    pipeline = Pipeline(stages=indexers + [assembler, scaler, gbt])
    
    print(f"âœ… Pipeline oluÅŸturuldu: {len(feature_columns)} Ã¶zellik")
    return pipeline

def train_and_evaluate_model(pipeline, train_df, val_df, test_df, target_column="total_daily_energy"):
    """
    Modeli eÄŸit ve deÄŸerlendir
    """
    print("\nğŸ¯ Model EÄŸitiliyor...")
    
    start_time = time.time()
    
    # Model eÄŸitimi
    model = pipeline.fit(train_df)
    
    training_time = time.time() - start_time
    print(f"âœ… Model eÄŸitimi tamamlandÄ±: {training_time:.1f} saniye")
    
    # Tahminler
    print("\nğŸ“Š Model DeÄŸerlendiriliyor...")
    
    train_predictions = model.transform(train_df)
    val_predictions = model.transform(val_df)
    test_predictions = model.transform(test_df)
    
    # Evaluator
    evaluator = RegressionEvaluator(
        labelCol=target_column,
        predictionCol="prediction"
    )
    
    # Metrikleri hesapla
    metrics = {}
    
    for dataset_name, predictions in [("train", train_predictions), 
                                     ("validation", val_predictions), 
                                     ("test", test_predictions)]:
        
        # RMSE
        evaluator.setMetricName("rmse")
        rmse = evaluator.evaluate(predictions)
        
        # R2
        evaluator.setMetricName("r2")
        r2 = evaluator.evaluate(predictions)
        
        # MAE
        evaluator.setMetricName("mae")
        mae = evaluator.evaluate(predictions)
        
        # MAPE (Manual calculation)
        mape_df = predictions.select(
            ((abs(col(target_column) - col("prediction")) / col(target_column)) * 100).alias("ape")
        )
        mape = mape_df.select(spark_avg("ape")).collect()[0][0]
        
        metrics[dataset_name] = {
            "rmse": rmse,
            "r2": r2,
            "mae": mae,
            "mape": mape
        }
        
        print(f"\n{dataset_name.upper()} Metrikleri:")
        print(f"  RMSE: {rmse:,.2f}")
        print(f"  RÂ²: {r2:.4f}")
        print(f"  MAE: {mae:,.2f}")
        print(f"  MAPE: {mape:.2f}%")
    
    return model, metrics, test_predictions

def predict_specific_date(model, spark, date_str, feature_columns, historical_df):
    """
    Belirli bir tarih iÃ§in tahmin yap
    Ã–rnek: "2016-03-05" iÃ§in tahmin
    """
    print(f"\nğŸ”® {date_str} Tarihi Ä°Ã§in Tahmin YapÄ±lÄ±yor...")
    
    # Tarih objesine Ã§evir
    target_date = datetime.strptime(date_str, "%Y-%m-%d").date()
    
    # GeÃ§miÅŸ verileri al (feature engineering iÃ§in)
    historical_data = historical_df.filter(col("date") < target_date).orderBy(col("date").desc())
    
    # Tahmin iÃ§in DataFrame oluÅŸtur
    prediction_df = spark.createDataFrame([(target_date,)], ["date"])
    
    # Zaman Ã¶zelliklerini ekle
    prediction_df = create_time_features(prediction_df)
    
    # Lag Ã¶zelliklerini historical_df'den al
    # Son 28 gÃ¼nlÃ¼k veriyi al
    recent_data = historical_data.limit(28).orderBy("date")
    
    # Lag deÄŸerlerini hesapla
    lag_values = {}
    for i in [1, 2, 3, 7, 14, 21, 28]:
        lag_row = recent_data.filter(col("date") == date_add(lit(target_date), -i)).select("total_daily_energy").collect()
        if lag_row:
            lag_values[f"energy_lag_{i}d"] = lag_row[0][0]
        else:
            lag_values[f"energy_lag_{i}d"] = 0
    
    # Hareketli ortalamalarÄ± hesapla
    for window_size in [3, 7, 14, 28]:
        ma_data = recent_data.filter(col("date") > date_add(lit(target_date), -window_size)) \
                            .select(spark_avg("total_daily_energy").alias("ma")).collect()
        if ma_data:
            lag_values[f"energy_ma_{window_size}d"] = ma_data[0][0]
        else:
            lag_values[f"energy_ma_{window_size}d"] = 0
    
    # DiÄŸer lag Ã¶zelliklerini ekle
    for col_name, value in lag_values.items():
        prediction_df = prediction_df.withColumn(col_name, lit(value))
    
    # Eksik kolonlarÄ± 0 ile doldur
    for col_name in feature_columns:
        if col_name not in prediction_df.columns:
            prediction_df = prediction_df.withColumn(col_name, lit(0))
    
    # Tahmin yap
    prediction = model.transform(prediction_df)
    
    # Sonucu al
    result = prediction.select("date", "prediction").collect()[0]
    predicted_energy = result["prediction"]
    
    print(f"âœ… Tahmin TamamlandÄ±!")
    print(f"ğŸ“… Tarih: {date_str}")
    print(f"âš¡ Tahmini Toplam Enerji TÃ¼ketimi: {predicted_energy:,.2f} birim")
    
    # GeÃ§miÅŸ ortalamalarla karÅŸÄ±laÅŸtÄ±r
    avg_weekday = historical_data.filter(col("day_of_week") == dayofweek(lit(target_date))) \
                                .select(spark_avg("total_daily_energy")).collect()[0][0]
    
    avg_monthly = historical_data.filter(col("month") == month(lit(target_date))) \
                                .select(spark_avg("total_daily_energy")).collect()[0][0]
    
    print(f"\nğŸ“Š KarÅŸÄ±laÅŸtÄ±rma:")
    print(f"   AynÄ± gÃ¼n tipi ortalamasÄ±: {avg_weekday:,.2f} birim")
    print(f"   AynÄ± ay ortalamasÄ±: {avg_monthly:,.2f} birim")
    print(f"   Tahmin farkÄ± (gÃ¼n tipi): {((predicted_energy/avg_weekday - 1) * 100):.1f}%")
    
    return predicted_energy

def save_model_and_artifacts(model, metrics, feature_columns, mlflow_run_id):
    """
    Model ve artifacts'larÄ± kaydet
    """
    print("\nğŸ’¾ Model ve Artifacts Kaydediliyor...")
    
    # Model Ã¶zeti
    model_summary = {
        "model_type": "GBTRegressor",
        "feature_count": len(feature_columns),
        "features": feature_columns,
        "metrics": metrics,
        "mlflow_run_id": mlflow_run_id,
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    
    # JSON olarak kaydet
    import tempfile
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        json.dump(model_summary, f, indent=2)
        mlflow.log_artifact(f.name, "model_summary.json")
    
    # Feature importance (eÄŸer varsa)
    if hasattr(model.stages[-1], 'featureImportances'):
        importances = model.stages[-1].featureImportances.toArray()
        feature_importance = list(zip(feature_columns, importances))
        feature_importance.sort(key=lambda x: x[1], reverse=True)
        
        print("\nğŸ† En Ã–nemli 10 Ã–zellik:")
        for i, (feature, importance) in enumerate(feature_importance[:10]):
            print(f"   {i+1}. {feature}: {importance:.4f}")
            mlflow.log_metric(f"feature_importance_{feature}", importance)
    
    print("âœ… Model ve artifacts kaydedildi")
    
    return model_summary

def main_daily_total_energy_pipeline():
    """
    Ana pipeline - GÃ¼nlÃ¼k toplam enerji tahmini
    """
    print("ğŸš€ GÃœNLÃœK TOPLAM ENERJÄ° TAHMÄ°NÄ° PIPELINE")
    print("="*80)
    
    # MLflow setup
    tracking_uri, experiment_name, experiment_id = setup_mlflow_daily_total()
    
    # MLflow run baÅŸlat
    with mlflow.start_run(run_name=f"Daily_Total_Energy_{datetime.now().strftime('%Y%m%d_%H%M%S')}") as run:
        
        print(f"ğŸ†” MLflow Run ID: {run.info.run_id}")
        
        try:
            
            spark = create_spark_session()
            mlflow.log_param("spark_version", spark.version)
            
            # Parametreler
            data_path = "/tmp/ultimate_full_year/*.csv*"  # Veri yolu
            test_days = 30  # Test iÃ§in son 30 gÃ¼n
            
            mlflow.log_param("data_path", data_path)
            mlflow.log_param("test_days", test_days)
            mlflow.log_param("prediction_type", "daily_total_energy")
            
            # 1. VERÄ° YÃœKLEME VE GÃœNLÃœK TOPLAM
            print("\n" + "="*50)
            print("ADIM 1: VERÄ° YÃœKLEME VE GÃœNLÃœK TOPLAMLAR")
            print("="*50)
            
            daily_df = load_and_aggregate_data(spark, data_path)
            
            mlflow.log_metric("total_days", daily_df.count())
            mlflow.log_metric("avg_daily_energy", 
                            daily_df.select(spark_avg("total_daily_energy")).collect()[0][0])
            
            # 2. Ã–ZELLÄ°K MÃœHENDÄ°SLÄ°ÄÄ°
            print("\n" + "="*50)
            print("ADIM 2: Ã–ZELLÄ°K MÃœHENDÄ°SLÄ°ÄÄ°")
            print("="*50)
            
            # Zaman Ã¶zellikleri
            daily_df = create_time_features(daily_df)
            
            # Lag Ã¶zellikleri
            daily_df = create_lag_features(daily_df)
            
            # Ä°leri dÃ¼zey Ã¶zellikler
            daily_df = create_advanced_features(daily_df)
            
            # Ã–zellik listesi
            feature_columns = [
                # Zaman Ã¶zellikleri
                "year", "month", "day_of_week", "day_of_month", "week_of_year", "quarter",
                "is_weekend", "is_monday", "is_friday", "season", "is_holiday",
                "is_month_start", "is_month_end",
                
                # DÃ¶ngÃ¼sel Ã¶zellikler
                "sin_day_of_week", "cos_day_of_week", "sin_day_of_month", "cos_day_of_month",
                "sin_month", "cos_month", "sin_week_of_year", "cos_week_of_year",
                
                # Lag Ã¶zellikleri
                "energy_lag_1d", "energy_lag_2d", "energy_lag_3d", "energy_lag_7d",
                "energy_lag_14d", "energy_lag_21d", "energy_lag_28d",
                
                # Hareketli ortalamalar
                "energy_ma_3d", "energy_ma_7d", "energy_ma_14d", "energy_ma_28d",
                
                # Hareketli standart sapma
                "energy_std_7d", "energy_std_14d",
                
                # Ã–zel lag'ler
                "energy_same_day_last_week", "energy_same_day_last_month",
                
                # Trend Ã¶zellikleri
                "energy_trend_7d", "energy_trend_28d",
                
                # YÃ¼k profili lag'leri
                "avg_load_lag_1d", "avg_load_lag_7d",
                "max_load_lag_1d", "max_load_lag_7d",
                "load_std_lag_1d", "load_std_lag_7d",
                
                # Ä°leri dÃ¼zey Ã¶zellikler
                "day_of_year", "days_since_month_start", "days_until_month_end",
                "consecutive_weekdays", "energy_zscore",
                
                # GÃ¼nlÃ¼k istatistikler
                "avg_load", "max_load", "min_load", "load_std",
                "high_load_count", "low_load_count", "reading_count"
            ]
            
            mlflow.log_param("num_features", len(feature_columns))
            mlflow.log_param("features", str(feature_columns))
            
            # 3. TRAIN/VALIDATION/TEST SPLIT
            print("\n" + "="*50)
            print("ADIM 3: VERÄ° BÃ–LME")
            print("="*50)
            
            train_df, val_df, test_df = prepare_train_test_split(daily_df, test_days)
            
            mlflow.log_metric("train_days", train_df.count())
            mlflow.log_metric("validation_days", val_df.count())
            mlflow.log_metric("test_days", test_df.count())
            
            # 4. MODEL PIPELINE
            print("\n" + "="*50)
            print("ADIM 4: MODEL EÄÄ°TÄ°MÄ°")
            print("="*50)
            
            # ML Pipeline oluÅŸtur
            pipeline = build_ml_pipeline(feature_columns, target_column="total_daily_energy")
            
            # Model parametrelerini logla
            mlflow.log_param("model_type", "GBTRegressor")
            mlflow.log_param("gbt_max_iter", 100)
            mlflow.log_param("gbt_max_depth", 8)
            mlflow.log_param("gbt_step_size", 0.1)
            mlflow.log_param("gbt_subsampling_rate", 0.8)
            
            # Model eÄŸitimi ve deÄŸerlendirme
            model, metrics, test_predictions = train_and_evaluate_model(
                pipeline, train_df, val_df, test_df
            )
            
            # Metrikleri MLflow'a logla
            for dataset_name, dataset_metrics in metrics.items():
                for metric_name, value in dataset_metrics.items():
                    mlflow.log_metric(f"{dataset_name}_{metric_name}", value)
            
            # Test performansÄ± Ã¶zeti
            print("\n" + "="*50)
            print("TEST PERFORMANSI Ã–ZETÄ°")
            print("="*50)
            print(f"ğŸ“Š Test RÂ²: {metrics['test']['r2']:.4f} ({metrics['test']['r2']*100:.2f}%)")
            print(f"ğŸ“Š Test RMSE: {metrics['test']['rmse']:,.2f}")
            print(f"ğŸ“Š Test MAPE: {metrics['test']['mape']:.2f}%")
            
            # Overfitting kontrolÃ¼
            overfitting_gap = metrics['train']['r2'] - metrics['test']['r2']
            mlflow.log_metric("overfitting_gap", overfitting_gap)
            print(f"ğŸ“Š Overfitting Gap: {overfitting_gap:.4f}")
            
            # BÃ¶lÃ¼m 5'te devam edecek...            
            # Spark session
            
# 5. Ã–RNEK TAHMÄ°NLER
            print("\n" + "="*50)
            print("ADIM 5: Ã–RNEK TAHMÄ°NLER")
            print("="*50)
            
            # Ã–rnek: 5 Mart 2016 tahmini
            example_date = "2016-03-05"
            predicted_energy = predict_specific_date(
                model, spark, example_date, feature_columns, daily_df
            )
            
            mlflow.log_metric("example_prediction", predicted_energy)
            mlflow.log_param("example_date", example_date)
            
            # BirkaÃ§ Ã¶rnek daha
            print("\nğŸ“… DiÄŸer Ã–rnek Tahminler:")
            for date_str in ["2016-01-15", "2016-06-20", "2016-09-10", "2016-12-25"]:
                try:
                    energy = predict_specific_date(
                        model, spark, date_str, feature_columns, daily_df
                    )
                    print(f"   {date_str}: {energy:,.2f} birim")
                except:
                    print(f"   {date_str}: Tahmin yapÄ±lamadÄ±")
            
            # 6. MODEL VE ARTIFACTS KAYDETME
            print("\n" + "="*50)
            print("ADIM 6: MODEL KAYDETME")
            print("="*50)
            
            # Model kaydet
            model_summary = save_model_and_artifacts(
                model, metrics, feature_columns, run.info.run_id
            )
            
            # MLflow model kaydÄ±
            model_info = mlflow.spark.log_model(
                spark_model=model,
                artifact_path="daily_total_energy_model",
                registered_model_name="Daily_Total_Energy_Forecasting_Model"
            )
            
            mlflow.log_param("model_uri", model_info.model_uri)
            
            # Test tahminlerini kaydet
            test_results = test_predictions.select(
                "date", "total_daily_energy", "prediction"
            ).withColumn(
                "error", col("total_daily_energy") - col("prediction")
            ).withColumn(
                "error_percent", 
                (abs(col("error")) / col("total_daily_energy")) * 100
            )
            
            # En iyi ve en kÃ¶tÃ¼ tahminler
            print("\nğŸ“ˆ En Ä°yi Tahminler (En DÃ¼ÅŸÃ¼k Hata):")
            test_results.orderBy("error_percent").show(5)
            
            print("\nğŸ“‰ En KÃ¶tÃ¼ Tahminler (En YÃ¼ksek Hata):")
            test_results.orderBy(col("error_percent").desc()).show(5)
            
            # Ã–zet
            print("\n" + "="*80)
            print("ğŸ‰ GÃœNLÃœK TOPLAM ENERJÄ° TAHMÄ°NÄ° BAÅARIYLA TAMAMLANDI!")
            print("="*80)
            print(f"âœ… Model URI: {model_info.model_uri}")
            print(f"âœ… MLflow Run ID: {run.info.run_id}")
            print(f"âœ… Test RÂ²: {metrics['test']['r2']*100:.2f}%")
            print(f"âœ… Test MAPE: {metrics['test']['mape']:.2f}%")
            print(f"âœ… Tracking URI: {tracking_uri}")
            
            return {
                "success": True,
                "model_uri": model_info.model_uri,
                "run_id": run.info.run_id,
                "metrics": metrics,
                "model_summary": model_summary
            }
            
        except Exception as e:
            mlflow.log_param("error", str(e))
            print(f"\nâŒ HATA: {e}")
            import traceback
            traceback.print_exc()
            raise
            
        finally:
            if 'spark' in locals():
                spark.stop()
                print("\nğŸ§¹ Spark session kapatÄ±ldÄ±")


# ANA FONKSÄ°YON
if __name__ == "__main__":
    print("ğŸš€ GÃœNLÃœK TOPLAM ENERJÄ° TAHMÄ°NÄ° BAÅLATILIYOR...")
    print("ğŸ“Š TÃ¼m sistem iÃ§in gÃ¼nlÃ¼k toplam enerji tÃ¼ketimi tahmini")
    print("="*80)
    
    try:
        # Pipeline'Ä± Ã§alÄ±ÅŸtÄ±r
        result = main_daily_total_energy_pipeline()
        
        if result["success"]:
            print("\n" + "="*80)
            print("âœ… TÃœM Ä°ÅLEMLER BAÅARIYLA TAMAMLANDI!")
            print("="*80)
            print("\nğŸ“Š Ã–ZET:")
            print(f"   - Model URI: {result['model_uri']}")
            print(f"   - Test RÂ²: {result['metrics']['test']['r2']*100:.2f}%")
            print(f"   - Test MAPE: {result['metrics']['test']['mape']:.2f}%")
            print(f"   - MLflow Run ID: {result['run_id']}")
            print("\nğŸ’¡ Model artÄ±k gÃ¼nlÃ¼k toplam enerji tahminleri yapabilir!")
            print("   Ã–rnek: '5 Mart 2016 tarihinde toplam ne kadar enerji kullanÄ±lacak?'")
            
    except Exception as e:
        print(f"\nâŒ Pipeline hatasÄ±: {e}")
        import traceback
        traceback.print_exc()            
            