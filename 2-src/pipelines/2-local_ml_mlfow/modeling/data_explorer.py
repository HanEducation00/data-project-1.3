#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Database explorer - MEVCUTdata_loader kullan
"""

import sys
import os

sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'data_processing'))

def explore_database_simple():
    """Mevcut data_loader ile explore et"""
    
    from data_loader import load_electricity_data
    
    print("ðŸ” Exploring database with existing data_loader...")
    
    # Ã‡ok bÃ¼yÃ¼k bir limit ile tÃ¼m veriyi yÃ¼klemeye Ã§alÄ±ÅŸ
    try:
        print("ðŸ“Š Loading large sample to estimate total size...")
        
        # 1 milyon kayÄ±t limit - database'in max'ini gÃ¶rmek iÃ§in
        raw_df, loader = load_electricity_data(
            start_date="2010-01-01",  # Ã‡ok eski tarih
            end_date="2030-12-31",    # Ã‡ok gelecek tarih
            limit=1000000             # 1M limit
        )
        
        total_loaded = raw_df.count()
        print(f"ðŸ“Š Loaded records: {total_loaded:,}")
        
        if total_loaded == 1000000:
            print("âš ï¸ Hit limit! Database has MORE than 1M records")
            print("ðŸ” Let's check date range...")
        
        # Date range check
        from pyspark.sql.functions import min as spark_min, max as spark_max, count, countDistinct
        
        date_stats = raw_df.agg(
            spark_min("full_timestamp").alias("min_date"),
            spark_max("full_timestamp").alias("max_date"),
            count("*").alias("total_records"),
            countDistinct("customer_id").alias("unique_customers"),
            countDistinct("full_timestamp").alias("unique_timestamps")
        ).collect()[0]
        
        min_date = date_stats["min_date"]
        max_date = date_stats["max_date"] 
        total_records = date_stats["total_records"]
        unique_customers = date_stats["unique_customers"]
        unique_timestamps = date_stats["unique_timestamps"]
        
        print(f"\nðŸ“… DATE RANGE:")
        print(f"   ðŸŸ¢ Start: {min_date}")
        print(f"   ðŸ”´ End: {max_date}")
        
        print(f"\nðŸ“Š STATS:")
        print(f"   ðŸ“ˆ Total records: {total_records:,}")
        print(f"   ðŸ‘¥ Unique customers: {unique_customers:,}")
        print(f"   â° Unique timestamps: {unique_timestamps:,}")
        
        # Calculate duration
        from datetime import datetime
        if isinstance(min_date, str):
            min_date = datetime.strptime(min_date, '%Y-%m-%d %H:%M:%S')
        if isinstance(max_date, str):
            max_date = datetime.strptime(max_date, '%Y-%m-%d %H:%M:%S')
            
        duration = max_date - min_date
        total_hours = int(duration.total_seconds() / 3600)
        total_days = int(total_hours / 24)
        
        print(f"   ðŸ“† Duration: {total_days:,} days ({total_hours:,} hours)")
        print(f"   ðŸ“ˆ Avg records/hour: {total_records / total_hours:.1f}")
        print(f"   ðŸ“ˆ Avg customers/hour: {total_records / unique_customers / total_hours * unique_customers:.1f}")
        
        # Year breakdown
        print(f"\nðŸ“… SAMPLING YEARS...")
        raw_df.createOrReplaceTempView("raw_data")
        
        year_stats = loader.spark.sql("""
            SELECT 
                YEAR(full_timestamp) as year,
                COUNT(*) as records,
                COUNT(DISTINCT customer_id) as customers
            FROM raw_data 
            GROUP BY YEAR(full_timestamp)
            ORDER BY year
        """).collect()
        
        print("ðŸ“Š YEAR BREAKDOWN (from sample):")
        for row in year_stats:
            print(f"   ðŸ“… {row['year']}: {row['records']:,} records, {row['customers']:,} customers")
        
        # Strategy recommendation
        print(f"\nðŸŽ¯ STRATEGY RECOMMENDATION:")
        
        if total_hours > 8760:  # 1+ year
            print("âœ… EXCELLENT! 1+ year data!")
            print("ðŸš€ FULL DATA TRAINING RECOMMENDED:")
            print(f"   ðŸ“ˆ Expected aggregated hours: {total_hours:,}")
            print(f"   ðŸ“Š Suggested train/val/test split:")
            print(f"      ðŸ‹ï¸ Train: {int(total_hours * 0.7):,} hours")
            print(f"      ðŸ“Š Validation: {int(total_hours * 0.15):,} hours") 
            print(f"      ðŸ§ª Test: {int(total_hours * 0.15):,} hours")
            
            print(f"\nðŸ”¥ ADVANCED FEATURES TO ADD:")
            print("   â€¢ Lag features: [1h, 6h, 24h, 168h]")
            print("   â€¢ Rolling features: [24h, 168h, 720h]") 
            print("   â€¢ Seasonal: month_sin/cos, hour_sin/cos")
            print("   â€¢ Calendar: is_weekend, is_holiday")
            
        elif total_hours > 2160:  # 3+ months
            print("âœ… GOOD! 3+ months data")
            print("ðŸ“Š Can capture weekly patterns")
        else:
            print("âš ï¸ Limited data")
        
        loader.close()
        
        return {
            'sample_records': total_records,
            'total_hours': total_hours,
            'total_days': total_days,
            'unique_customers': unique_customers,
            'date_range': (min_date, max_date),
            'hit_limit': total_loaded == 1000000
        }
        
    except Exception as e:
        print(f"âŒ Exploration failed: {e}")
        return None

def estimate_full_size():
    """Tam boyutu tahmin et"""
    
    print("\nðŸ” ESTIMATING FULL DATABASE SIZE...")
    
    # KÃ¼Ã§Ã¼k sample al
    from data_loader import load_electricity_data
    
    raw_df, loader = load_electricity_data(
        start_date="2016-01-01",
        end_date="2016-01-02", 
        limit=10000
    )
    
    sample_count = raw_df.count()
    
    # 1 gÃ¼nlÃ¼k sample'dan tahmin
    if sample_count > 0:
        print(f"ðŸ“Š 1-day sample: {sample_count:,} records")
        
        # Date range al
        from pyspark.sql.functions import min as spark_min, max as spark_max
        date_range = raw_df.agg(
            spark_min("full_timestamp").alias("min_date"),
            spark_max("full_timestamp").alias("max_date")
        ).collect()[0]
        
        min_date = date_range["min_date"]
        max_date = date_range["max_date"]
        
        from datetime import datetime
        if isinstance(min_date, str):
            min_date = datetime.strptime(min_date, '%Y-%m-%d %H:%M:%S')
        if isinstance(max_date, str):
            max_date = datetime.strptime(max_date, '%Y-%m-%d %H:%M:%S')
            
        sample_hours = (max_date - min_date).total_seconds() / 3600
        records_per_hour = sample_count / sample_hours if sample_hours > 0 else 0
        
        print(f"ðŸ“Š Sample duration: {sample_hours:.1f} hours")
        print(f"ðŸ“Š Records per hour: {records_per_hour:.1f}")
        
        # Estimate for different periods
        estimates = {
            "1 week": int(records_per_hour * 24 * 7),
            "1 month": int(records_per_hour * 24 * 30),
            "1 year": int(records_per_hour * 24 * 365)
        }
        
        print(f"\nðŸ“Š SIZE ESTIMATES:")
        for period, estimate in estimates.items():
            print(f"   {period}: ~{estimate:,} records")
    
    loader.close()

if __name__ == "__main__":
    print("ðŸ§ª DATABASE EXPLORATION")
    
    # Ä°lk bÃ¼yÃ¼k sample
    result = explore_database_simple()
    
    if result and result.get('hit_limit'):
        print("\n" + "="*50)
        estimate_full_size()
    
    print("\nðŸŽ¯ READY FOR FULL DATA TRAINING!")
